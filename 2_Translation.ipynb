{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is part of the Deep Learning class\n",
    "\n",
    "In this notebook, I train an RNN model (LSTM and bi-LSTM models) to translate from French and Italian to English using letter by letter technique.\n",
    "\n",
    "The dataset is downloaded from: \"http://www.manythings.org/anki/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Load and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., filename = 'Data/deu.txt'\n",
    "filename_fra = 'Data/fra.txt'\n",
    "filename_ita = 'Data/ita.txt'\n",
    "\n",
    "# e.g., n_train = 20000\n",
    "n_train = 160003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "doc_fra = load_doc(filename_fra)\n",
    "doc_ita = load_doc(filename_ita)\n",
    "\n",
    "# split into Language1-Language2 pairs\n",
    "pairs_fra = to_pairs(doc_fra)\n",
    "pairs_ita = to_pairs(doc_ita)\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs_fra = clean_data(pairs_fra)[:n_train, :]\n",
    "clean_pairs_ita = clean_data(pairs_ita)[:n_train, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[whose is it] => [a qui estce]\n",
      "[whose is it] => [cest a qui]\n",
      "[will it fit] => [cela iratil]\n",
      "[will it fit] => [cela passeratil]\n",
      "[will it fit] => [cela sadapteratil]\n",
      "[will you go] => [irezvous]\n",
      "[will you go] => [irastu]\n",
      "[will you go] => [ty rendrastu]\n",
      "[will you go] => [vous y rendrezvous]\n",
      "[work slowly] => [travaille lentement]\n",
      "[im flabby] => [io sono fiacco]\n",
      "[im flabby] => [sono fiacca]\n",
      "[im flabby] => [io sono fiacca]\n",
      "[im for it] => [sono a favore]\n",
      "[im for it] => [io sono a favore]\n",
      "[im frugal] => [sono parsimonioso]\n",
      "[im greedy] => [sono avido]\n",
      "[im greedy] => [io sono avido]\n",
      "[im greedy] => [sono avida]\n",
      "[im greedy] => [io sono avida]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs_fra[i, 0] + '] => [' + clean_pairs_fra[i, 1] + ']')\n",
    "    \n",
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs_ita[i, 0] + '] => [' + clean_pairs_ita[i, 1] + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts french :  (160003,)\n",
      "Length of target_texts french: (160003,)\n",
      "Length of input_texts italian :  (160003,)\n",
      "Length of target_texts italian: (160003,)\n"
     ]
    }
   ],
   "source": [
    "input_texts_fra  = clean_pairs_fra[:, 0]\n",
    "target_texts_fra = ['\\t' + text + '\\n' for text in clean_pairs_fra[:, 1]]\n",
    "\n",
    "print('Length of input_texts french :  ' + str(input_texts_fra.shape))\n",
    "print('Length of target_texts french: ' + str(input_texts_fra.shape))\n",
    "\n",
    "\n",
    "input_texts_ita  = clean_pairs_ita[:, 0]\n",
    "target_texts_ita = ['\\t' + text + '\\n' for text in clean_pairs_ita[:, 1]]\n",
    "\n",
    "print('Length of input_texts italian :  ' + str(input_texts_ita.shape))\n",
    "print('Length of target_texts italian: ' + str(input_texts_ita.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of input  sentences french: 54\n",
      "max length of target sentences french: 119\n",
      "max length of input  sentences italian: 25\n",
      "max length of target sentences italian: 102\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_length_fra = max(len(line) for line in input_texts_fra)\n",
    "max_decoder_seq_length_fra = max(len(line) for line in target_texts_fra)\n",
    "\n",
    "print('max length of input  sentences french: %d' % (max_encoder_seq_length_fra))\n",
    "print('max length of target sentences french: %d' % (max_decoder_seq_length_fra))\n",
    "\n",
    "max_encoder_seq_length_ita = max(len(line) for line in input_texts_ita)\n",
    "max_decoder_seq_length_ita = max(len(line) for line in target_texts_ita)\n",
    "\n",
    "print('max length of input  sentences italian: %d' % (max_encoder_seq_length_ita))\n",
    "print('max length of target sentences italian: %d' % (max_decoder_seq_length_ita))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Convert texts to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq French : (160003, 54)\n",
      "shape of input_token_index French : 27\n",
      "shape of decoder_input_seq French : (160003, 119)\n",
      "shape of target_token_index French: 29\n",
      "shape of encoder_input_seq Italian : (160003, 25)\n",
      "shape of input_token_index Italian : 27\n",
      "shape of decoder_input_seq Italian : (160003, 102)\n",
      "shape of target_token_index Italian: 29\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# encode and pad sequences\n",
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index\n",
    "\n",
    "encoder_input_seq_fra, input_token_index_fra  = text2sequences(max_encoder_seq_length_fra, input_texts_fra)\n",
    "decoder_input_seq_fra, target_token_index_fra = text2sequences(max_decoder_seq_length_fra, target_texts_fra)\n",
    "\n",
    "print('shape of encoder_input_seq French : ' + str(encoder_input_seq_fra.shape))\n",
    "print('shape of input_token_index French : ' + str(len(input_token_index_fra)))\n",
    "print('shape of decoder_input_seq French : ' + str(decoder_input_seq_fra.shape))\n",
    "print('shape of target_token_index French: ' + str(len(target_token_index_fra)))\n",
    "\n",
    "encoder_input_seq_ita, input_token_index_ita  = text2sequences(max_encoder_seq_length_ita, input_texts_ita)\n",
    "decoder_input_seq_ita, target_token_index_ita = text2sequences(max_decoder_seq_length_ita, target_texts_ita)\n",
    "\n",
    "print('shape of encoder_input_seq Italian : ' + str(encoder_input_seq_ita.shape))\n",
    "print('shape of input_token_index Italian : ' + str(len(input_token_index_ita)))\n",
    "print('shape of decoder_input_seq Italian : ' + str(decoder_input_seq_ita.shape))\n",
    "print('shape of target_token_index Italian: ' + str(len(target_token_index_ita)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens French: 28\n",
      "num_decoder_tokens French: 30\n",
      "num_encoder_tokens Italian: 28\n",
      "num_decoder_tokens Italian: 30\n"
     ]
    }
   ],
   "source": [
    "# the 1 is added to add a class that there are no more letters\n",
    "num_encoder_tokens_fra = len(input_token_index_fra) + 1\n",
    "num_decoder_tokens_fra = len(target_token_index_fra) + 1\n",
    "\n",
    "print('num_encoder_tokens French: ' + str(num_encoder_tokens_fra))\n",
    "print('num_decoder_tokens French: ' + str(num_decoder_tokens_fra))\n",
    "\n",
    "# the 1 is added to add a class that there are no more letters\n",
    "num_encoder_tokens_ita = len(input_token_index_ita) + 1\n",
    "num_decoder_tokens_ita = len(target_token_index_ita) + 1\n",
    "\n",
    "print('num_encoder_tokens Italian: ' + str(num_encoder_tokens_ita))\n",
    "print('num_decoder_tokens Italian: ' + str(num_decoder_tokens_ita))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tentrez\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts_fra[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tlavoro ai ferri\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts_ita[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12  2  8  7  9  2 25 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[10 12  3 18  2  8  2  1  3  5  1 22  4  8  8  5 11  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_seq_fra[100, :])\n",
    "print(decoder_input_seq_ita[100, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_data French:  (160003, 54, 28)\n",
      "decoder_input_data French:  (160003, 119, 30)\n",
      "encoder_input_data Italian:  (160003, 25, 28)\n",
      "decoder_input_data Italian:  (160003, 102, 30)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encode target sequence\n",
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences) # number of sentences\n",
    "    data = numpy.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data\n",
    "\n",
    "encoder_input_data_fra = onehot_encode(encoder_input_seq_fra, max_encoder_seq_length_fra, num_encoder_tokens_fra)\n",
    "decoder_input_data_fra = onehot_encode(decoder_input_seq_fra, max_decoder_seq_length_fra, num_decoder_tokens_fra)\n",
    "\n",
    "decoder_target_seq_fra = numpy.zeros(decoder_input_seq_fra.shape)\n",
    "decoder_target_seq_fra[:, 0:-1] = decoder_input_seq_fra[:, 1:]\n",
    "decoder_target_data_fra = onehot_encode(decoder_target_seq_fra, \n",
    "                                    max_decoder_seq_length_fra, \n",
    "                                    num_decoder_tokens_fra)\n",
    "\n",
    "print('encoder_input_data French: ', encoder_input_data_fra.shape)\n",
    "print('decoder_input_data French: ', decoder_input_data_fra.shape)\n",
    "\n",
    "\n",
    "encoder_input_data_ita = onehot_encode(encoder_input_seq_ita, max_encoder_seq_length_ita, num_encoder_tokens_ita)\n",
    "decoder_input_data_ita = onehot_encode(decoder_input_seq_ita, max_decoder_seq_length_ita, num_decoder_tokens_ita)\n",
    "\n",
    "decoder_target_seq_ita = numpy.zeros(decoder_input_seq_ita.shape)\n",
    "decoder_target_seq_ita[:, 0:-1] = decoder_input_seq_ita[:, 1:]\n",
    "decoder_target_data_ita = onehot_encode(decoder_target_seq_ita, \n",
    "                                    max_decoder_seq_length_ita, \n",
    "                                    num_decoder_tokens_ita)\n",
    "\n",
    "print('encoder_input_data Italian: ', encoder_input_data_ita.shape)\n",
    "print('decoder_input_data Italian: ', decoder_input_data_ita.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Divide Data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_input_data_fra  = encoder_input_data_fra[:159000]\n",
    "train_decoder_input_data_fra  = decoder_input_data_fra[:159000]\n",
    "train_decoder_target_data_fra = decoder_target_data_fra[:159000]\n",
    "\n",
    "test_encoder_input_data_fra  = encoder_input_data_fra[159000:]\n",
    "test_decoder_input_data_fra  = decoder_input_data_fra[159000:]\n",
    "test_decoder_target_data_fra = decoder_target_data_fra[159000:]\n",
    "\n",
    "train_encoder_input_data_ita  = encoder_input_data_ita[:159000]\n",
    "train_decoder_input_data_ita  = decoder_input_data_ita[:159000]\n",
    "train_decoder_target_data_ita = decoder_target_data_ita[:159000]\n",
    "\n",
    "test_encoder_input_data_ita  = encoder_input_data_ita[159000:]\n",
    "test_decoder_input_data_ita  = decoder_input_data_ita[159000:]\n",
    "test_decoder_target_data_ita = decoder_target_data_ita[159000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Build the networks (for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Bidirectional, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs_fra = Input(shape=(None, num_encoder_tokens_fra), name='encoder_inputs_fra')\n",
    "encoder_inputs_ita = Input(shape=(None, num_encoder_tokens_ita), name='encoder_inputs_ita')\n",
    "\n",
    "# set the BiLSTM layer\n",
    "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True,\n",
    "                                  dropout=0.5, name='encoder_bilstm'))\n",
    "_, forward_h_fra, forward_c_fra, backward_h_fra, backward_c_fra = encoder_bilstm(encoder_inputs_fra)\n",
    "_, forward_h_ita, forward_c_ita, backward_h_ita, backward_c_ita = encoder_bilstm(encoder_inputs_ita)\n",
    "\n",
    "state_h_fra = Concatenate()([forward_h_fra, backward_h_fra])\n",
    "state_c_fra = Concatenate()([forward_c_fra, backward_c_fra])\n",
    "\n",
    "state_h_ita = Concatenate()([forward_h_ita, backward_h_ita])\n",
    "state_c_ita = Concatenate()([forward_c_ita, backward_c_ita])\n",
    "\n",
    "# build the encoder network model\n",
    "encoder_model = Model(inputs=[encoder_inputs_fra, encoder_inputs_ita], \n",
    "                      outputs=[state_h_fra, state_c_fra, state_h_ita, state_c_ita],\n",
    "                      name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs_fra (InputLayer) (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_inputs_ita (InputLayer) (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  583680      encoder_inputs_fra[0][0]         \n",
      "                                                                 encoder_inputs_ita[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           bidirectional_1[1][1]            \n",
      "                                                                 bidirectional_1[1][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           bidirectional_1[1][2]            \n",
      "                                                                 bidirectional_1[1][4]            \n",
      "==================================================================================================\n",
      "Total params: 583,680\n",
      "Trainable params: 583,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Decoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# inputs of the decoder network\n",
    "decoder_input_h_fra = Input(shape=(2*latent_dim,), name='decoder_input_h_fra')\n",
    "decoder_input_c_fra = Input(shape=(2*latent_dim,), name='decoder_input_c_fra')\n",
    "decoder_input_x_fra = Input(shape=(None, num_decoder_tokens_fra), name='decoder_input_x_fra')\n",
    "\n",
    "decoder_input_h_ita = Input(shape=(2*latent_dim,), name='decoder_input_h_ita')\n",
    "decoder_input_c_ita = Input(shape=(2*latent_dim,), name='decoder_input_c_ita')\n",
    "decoder_input_x_ita = Input(shape=(None, num_decoder_tokens_fra), name='decoder_input_x_ita')\n",
    "\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm_fra = LSTM(latent_dim*2, return_sequences=True, \n",
    "                    return_state=True, dropout=0.25, name='decoder_lstm_fra')\n",
    "decoder_lstm_outputs_fra, state_h_fra, state_c_fra = decoder_lstm_fra(decoder_input_x_fra, \n",
    "                                                      initial_state=[decoder_input_h_fra, decoder_input_c_fra])\n",
    "\n",
    "decoder_lstm_ita = LSTM(latent_dim*2, return_sequences=True, \n",
    "                    return_state=True, dropout=0.25, name='decoder_lstm_ita')\n",
    "decoder_lstm_outputs_ita, state_h_ita, state_c_ita = decoder_lstm_ita(decoder_input_x_ita, \n",
    "                                                      initial_state=[decoder_input_h_ita, decoder_input_c_ita])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense_fra   = Dense(num_decoder_tokens_fra, activation='softmax', name='decoder_dense_fra')\n",
    "decoder_outputs_fra = decoder_dense_fra(decoder_lstm_outputs_fra)\n",
    "\n",
    "decoder_dense_ita   = Dense(num_decoder_tokens_ita, activation='softmax', name='decoder_dense_ita')\n",
    "decoder_outputs_ita = decoder_dense_ita(decoder_lstm_outputs_ita)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model = Model(inputs=[decoder_input_x_fra, decoder_input_h_fra, decoder_input_c_fra, decoder_input_x_ita, decoder_input_h_ita, decoder_input_c_ita],\n",
    "                      outputs=[decoder_outputs_fra, state_h_fra, state_c_fra, decoder_outputs_ita, state_h_ita, state_c_ita],\n",
    "                      name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x_fra (InputLayer (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h_fra (InputLayer (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c_fra (InputLayer (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x_ita (InputLayer (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h_ita (InputLayer (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c_ita (InputLayer (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_fra (LSTM)         [(None, None, 512),  1112064     decoder_input_x_fra[0][0]        \n",
      "                                                                 decoder_input_h_fra[0][0]        \n",
      "                                                                 decoder_input_c_fra[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_ita (LSTM)         [(None, None, 512),  1112064     decoder_input_x_ita[0][0]        \n",
      "                                                                 decoder_input_h_ita[0][0]        \n",
      "                                                                 decoder_input_c_ita[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense_fra (Dense)       (None, None, 30)     15390       decoder_lstm_fra[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense_ita (Dense)       (None, None, 30)     15390       decoder_lstm_ita[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 2,254,908\n",
      "Trainable params: 2,254,908\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. Connect the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layers\n",
    "encoder_input_x_fra = Input(shape=(None, num_encoder_tokens_fra), name='encoder_input_x_fra')\n",
    "decoder_input_x_fra = Input(shape=(None, num_decoder_tokens_fra), name='decoder_input_x_fra')\n",
    "\n",
    "encoder_input_x_ita = Input(shape=(None, num_encoder_tokens_ita), name='encoder_input_x_ita')\n",
    "decoder_input_x_ita = Input(shape=(None, num_decoder_tokens_ita), name='decoder_input_x_ita')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x_fra, encoder_input_x_fra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layers\n",
    "encoder_input_x_fra = Input(shape=(None, num_encoder_tokens_fra), name='encoder_input_x_fra')\n",
    "decoder_input_x_fra = Input(shape=(None, num_decoder_tokens_fra), name='decoder_input_x_fra')\n",
    "\n",
    "encoder_input_x_ita = Input(shape=(None, num_encoder_tokens_ita), name='encoder_input_x_ita')\n",
    "decoder_input_x_ita = Input(shape=(None, num_decoder_tokens_ita), name='decoder_input_x_ita')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x_fra, encoder_input_x_ita])\n",
    "\n",
    "decoder_lstm_output_fra, _, _ = decoder_lstm_fra(decoder_input_x_fra, initial_state=encoder_final_states[:2])\n",
    "decoder_pred_fra = decoder_dense_fra(decoder_lstm_output_fra)\n",
    "\n",
    "decoder_lstm_output_ita, _, _ = decoder_lstm_ita(decoder_input_x_ita, initial_state=encoder_final_states[2:])\n",
    "decoder_pred_ita = decoder_dense_ita(decoder_lstm_output_ita)\n",
    "\n",
    "model = Model(inputs=[encoder_input_x_fra, encoder_input_x_ita, decoder_input_x_fra, decoder_input_x_ita], \n",
    "              outputs=[decoder_pred_fra, decoder_pred_ita], \n",
    "              name='model_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_x_fra (InputLayer (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_input_x_ita (InputLayer (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x_fra (InputLayer (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 512), (None, 583680      encoder_input_x_fra[0][0]        \n",
      "                                                                 encoder_input_x_ita[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x_ita (InputLayer (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_fra (LSTM)         [(None, None, 512),  1112064     decoder_input_x_fra[0][0]        \n",
      "                                                                 encoder[2][0]                    \n",
      "                                                                 encoder[2][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_ita (LSTM)         [(None, None, 512),  1112064     decoder_input_x_ita[0][0]        \n",
      "                                                                 encoder[2][2]                    \n",
      "                                                                 encoder[2][3]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense_fra (Dense)       (None, None, 30)     15390       decoder_lstm_fra[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense_ita (Dense)       (None, None, 30)     15390       decoder_lstm_ita[1][0]           \n",
      "==================================================================================================\n",
      "Total params: 2,838,588\n",
      "Trainable params: 2,838,588\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5. Fit the model on the bilingual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_data(160003, 54, 28)\n",
      "shape of decoder_input_data(160003, 119, 30)\n",
      "shape of decoder_target_data(160003, 119, 30)\n",
      "shape of encoder_input_data(160003, 25, 28)\n",
      "shape of decoder_input_data(160003, 102, 30)\n",
      "shape of decoder_target_data(160003, 102, 30)\n"
     ]
    }
   ],
   "source": [
    "print('shape of encoder_input_data' + str(encoder_input_data_fra.shape))\n",
    "print('shape of decoder_input_data' + str(decoder_input_data_fra.shape))\n",
    "print('shape of decoder_target_data' + str(decoder_target_data_fra.shape))\n",
    "\n",
    "print('shape of encoder_input_data' + str(encoder_input_data_ita.shape))\n",
    "print('shape of decoder_input_data' + str(decoder_input_data_ita.shape))\n",
    "print('shape of decoder_target_data' + str(decoder_target_data_ita.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143100 samples, validate on 15900 samples\n",
      "Epoch 1/20\n",
      "143100/143100 [==============================] - 1508s 11ms/step - loss: 1.1484 - decoder_dense_fra_loss: 0.6284 - decoder_dense_ita_loss: 0.5200 - val_loss: 1.3933 - val_decoder_dense_fra_loss: 0.8653 - val_decoder_dense_ita_loss: 0.5280\n",
      "Epoch 2/20\n",
      "143100/143100 [==============================] - 1479s 10ms/step - loss: 0.8870 - decoder_dense_fra_loss: 0.4876 - decoder_dense_ita_loss: 0.3994 - val_loss: 1.2377 - val_decoder_dense_fra_loss: 0.7777 - val_decoder_dense_ita_loss: 0.4601\n",
      "Epoch 3/20\n",
      "143100/143100 [==============================] - 1478s 10ms/step - loss: 0.7925 - decoder_dense_fra_loss: 0.4364 - decoder_dense_ita_loss: 0.3561 - val_loss: 1.1314 - val_decoder_dense_fra_loss: 0.7172 - val_decoder_dense_ita_loss: 0.4143\n",
      "Epoch 4/20\n",
      "143100/143100 [==============================] - 1479s 10ms/step - loss: 0.7262 - decoder_dense_fra_loss: 0.4009 - decoder_dense_ita_loss: 0.3252 - val_loss: 1.0458 - val_decoder_dense_fra_loss: 0.6656 - val_decoder_dense_ita_loss: 0.3802\n",
      "Epoch 5/20\n",
      "143100/143100 [==============================] - 1478s 10ms/step - loss: 0.6762 - decoder_dense_fra_loss: 0.3744 - decoder_dense_ita_loss: 0.3018 - val_loss: 0.9880 - val_decoder_dense_fra_loss: 0.6310 - val_decoder_dense_ita_loss: 0.3571\n",
      "Epoch 6/20\n",
      "143100/143100 [==============================] - 1480s 10ms/step - loss: 0.6365 - decoder_dense_fra_loss: 0.3534 - decoder_dense_ita_loss: 0.2832 - val_loss: 0.9402 - val_decoder_dense_fra_loss: 0.6053 - val_decoder_dense_ita_loss: 0.3349\n",
      "Epoch 7/20\n",
      "143100/143100 [==============================] - 1478s 10ms/step - loss: 0.6041 - decoder_dense_fra_loss: 0.3364 - decoder_dense_ita_loss: 0.2677 - val_loss: 0.9000 - val_decoder_dense_fra_loss: 0.5801 - val_decoder_dense_ita_loss: 0.3199\n",
      "Epoch 8/20\n",
      "143100/143100 [==============================] - 1479s 10ms/step - loss: 0.5765 - decoder_dense_fra_loss: 0.3219 - decoder_dense_ita_loss: 0.2546 - val_loss: 0.8682 - val_decoder_dense_fra_loss: 0.5601 - val_decoder_dense_ita_loss: 0.3080\n",
      "Epoch 9/20\n",
      "143100/143100 [==============================] - 1479s 10ms/step - loss: 0.5531 - decoder_dense_fra_loss: 0.3099 - decoder_dense_ita_loss: 0.2433 - val_loss: 0.8374 - val_decoder_dense_fra_loss: 0.5427 - val_decoder_dense_ita_loss: 0.2947\n",
      "Epoch 10/20\n",
      "143100/143100 [==============================] - 1479s 10ms/step - loss: 0.5328 - decoder_dense_fra_loss: 0.2994 - decoder_dense_ita_loss: 0.2334 - val_loss: 0.8158 - val_decoder_dense_fra_loss: 0.5321 - val_decoder_dense_ita_loss: 0.2837\n",
      "Epoch 11/20\n",
      "143100/143100 [==============================] - 1480s 10ms/step - loss: 0.5151 - decoder_dense_fra_loss: 0.2904 - decoder_dense_ita_loss: 0.2247 - val_loss: 0.7930 - val_decoder_dense_fra_loss: 0.5173 - val_decoder_dense_ita_loss: 0.2757\n",
      "Epoch 12/20\n",
      "143100/143100 [==============================] - 1479s 10ms/step - loss: 0.4992 - decoder_dense_fra_loss: 0.2823 - decoder_dense_ita_loss: 0.2169 - val_loss: 0.7724 - val_decoder_dense_fra_loss: 0.5057 - val_decoder_dense_ita_loss: 0.2668\n",
      "Epoch 13/20\n",
      "143100/143100 [==============================] - 1480s 10ms/step - loss: 0.4851 - decoder_dense_fra_loss: 0.2751 - decoder_dense_ita_loss: 0.2100 - val_loss: 0.7571 - val_decoder_dense_fra_loss: 0.4965 - val_decoder_dense_ita_loss: 0.2606\n",
      "Epoch 14/20\n",
      "143100/143100 [==============================] - 1480s 10ms/step - loss: 0.4723 - decoder_dense_fra_loss: 0.2686 - decoder_dense_ita_loss: 0.2036 - val_loss: 0.7425 - val_decoder_dense_fra_loss: 0.4863 - val_decoder_dense_ita_loss: 0.2563\n",
      "Epoch 15/20\n",
      "143100/143100 [==============================] - 1479s 10ms/step - loss: 0.4608 - decoder_dense_fra_loss: 0.2628 - decoder_dense_ita_loss: 0.1980 - val_loss: 0.7301 - val_decoder_dense_fra_loss: 0.4810 - val_decoder_dense_ita_loss: 0.2491\n",
      "Epoch 16/20\n",
      "143100/143100 [==============================] - 1482s 10ms/step - loss: 0.4504 - decoder_dense_fra_loss: 0.2576 - decoder_dense_ita_loss: 0.1928 - val_loss: 0.7154 - val_decoder_dense_fra_loss: 0.4717 - val_decoder_dense_ita_loss: 0.2437\n",
      "Epoch 17/20\n",
      "143100/143100 [==============================] - 1481s 10ms/step - loss: 0.4407 - decoder_dense_fra_loss: 0.2527 - decoder_dense_ita_loss: 0.1881 - val_loss: 0.7076 - val_decoder_dense_fra_loss: 0.4664 - val_decoder_dense_ita_loss: 0.2412\n",
      "Epoch 18/20\n",
      "143100/143100 [==============================] - 1482s 10ms/step - loss: 0.4316 - decoder_dense_fra_loss: 0.2482 - decoder_dense_ita_loss: 0.1834 - val_loss: 0.6987 - val_decoder_dense_fra_loss: 0.4615 - val_decoder_dense_ita_loss: 0.2371\n",
      "Epoch 19/20\n",
      "143100/143100 [==============================] - 1481s 10ms/step - loss: 0.4233 - decoder_dense_fra_loss: 0.2440 - decoder_dense_ita_loss: 0.1793 - val_loss: 0.6871 - val_decoder_dense_fra_loss: 0.4548 - val_decoder_dense_ita_loss: 0.2322\n",
      "Epoch 20/20\n",
      "143100/143100 [==============================] - 1482s 10ms/step - loss: 0.4155 - decoder_dense_fra_loss: 0.2402 - decoder_dense_ita_loss: 0.1754 - val_loss: 0.6822 - val_decoder_dense_fra_loss: 0.4506 - val_decoder_dense_ita_loss: 0.2317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm_fra was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_1/concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'encoder_1/concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm_ita was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_1/concatenate_3/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'encoder_1/concatenate_4/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=1E-4),\n",
    "              loss=['categorical_crossentropy', 'categorical_crossentropy'])\n",
    "\n",
    "model.fit([train_encoder_input_data_fra, train_encoder_input_data_ita, train_decoder_input_data_fra, train_decoder_input_data_ita],  # training data\n",
    "          [train_decoder_target_data_fra, train_decoder_target_data_ita],                       # labels (left shift of the target sequences)\n",
    "          batch_size=32, epochs = 20, validation_split=0.1)\n",
    "\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Translate English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index_fra  = dict((i, char) for char, i in input_token_index_fra.items())\n",
    "reverse_target_char_index_fra = dict((i, char) for char, i in target_token_index_fra.items())\n",
    "\n",
    "reverse_input_char_index_ita  = dict((i, char) for char, i in input_token_index_ita.items())\n",
    "reverse_target_char_index_ita = dict((i, char) for char, i in target_token_index_ita.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq_fra, input_seq_ita):\n",
    "    states_value = encoder_model.predict([input_seq_fra, input_seq_ita])\n",
    "        \n",
    "    target_seq_fra = numpy.zeros((1, 1, num_decoder_tokens_fra))\n",
    "    target_seq_fra[0, 0, target_token_index_fra['\\t']] = 1.\n",
    "\n",
    "    target_seq_ita = numpy.zeros((1, 1, num_decoder_tokens_ita))\n",
    "    target_seq_ita[0, 0, target_token_index_ita['\\t']] = 1.\n",
    "        \n",
    "    stop_condition_fra = False\n",
    "    decoded_sentence_fra = ''\n",
    "    \n",
    "    stop_condition_ita = False\n",
    "    decoded_sentence_ita = ''\n",
    "    \n",
    "    while not stop_condition_fra:\n",
    "\n",
    "        output_tokens_fra, h_fra, c_fra, output_tokens_ita, h_ita, c_ita = decoder_model.predict([target_seq_fra, states_value[0], states_value[1], target_seq_ita, states_value[2], states_value[3]])\n",
    "        \n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        sampled_token_index = numpy.argmax(output_tokens_fra[0, -1, :])\n",
    "        \n",
    "        sampled_char = reverse_target_char_index_fra[sampled_token_index]\n",
    "        decoded_sentence_fra += sampled_char\n",
    "        \n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence_fra) > max_decoder_seq_length_fra):\n",
    "            stop_condition_fra = True\n",
    "\n",
    "        target_seq_fra = numpy.zeros((1, 1, num_decoder_tokens_fra))\n",
    "        target_seq_fra[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h_fra, c_fra, h_ita, c_ita]\n",
    "    \n",
    "    states_value = encoder_model.predict([input_seq_fra, input_seq_ita])\n",
    "    \n",
    "    while not stop_condition_ita:\n",
    "        output_tokens_fra, h_fra, c_fra, output_tokens_ita, h_ita, c_ita = decoder_model.predict([target_seq_fra, states_value[0], states_value[1], target_seq_ita, states_value[2], states_value[3]])\n",
    "        \n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        sampled_token_index = numpy.argmax(output_tokens_ita[0, -1, :])\n",
    "        \n",
    "        sampled_char = reverse_target_char_index_ita[sampled_token_index]\n",
    "        decoded_sentence_ita += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence_ita) > max_decoder_seq_length_ita):\n",
    "            stop_condition_ita = True\n",
    "\n",
    "        target_seq_ita = numpy.zeros((1, 1, num_decoder_tokens_ita))\n",
    "        target_seq_ita[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h_fra, c_fra, h_ita, c_ita]\n",
    "\n",
    "    return decoded_sentence_fra, decoded_sentence_ita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Translate an English sentence to the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence is            : why is that\n",
      "translated sentence French is : pourquoi le change\n",
      "\n",
      "translated sentence Italian is: perche e la stanza\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def token_data_fra(sentence):\n",
    "    cc = clean_data(sentence)\n",
    "    input_sequence = numpy.zeros((1, max_encoder_seq_length_fra))\n",
    "    for n in range(len(cc)):\n",
    "        if cc[n,0] == '':\n",
    "            cc[n,0] = ' '\n",
    "        input_sequence[0,n] = input_token_index_fra[cc[n,0]]\n",
    "    return input_sequence\n",
    "\n",
    "def token_data_ita(sentence):\n",
    "    cc = clean_data(sentence)\n",
    "    input_sequence = numpy.zeros((1, max_encoder_seq_length_ita))\n",
    "    for n in range(len(cc)):\n",
    "        if cc[n,0] == '':\n",
    "            cc[n,0] = ' '\n",
    "        input_sequence[0,n] = input_token_index_ita[cc[n,0]]\n",
    "    return input_sequence\n",
    "\n",
    "input_sentence = 'why is that'\n",
    "\n",
    "input_sequence_fra = token_data_fra(input_sentence)\n",
    "input_sequence_ita = token_data_ita(input_sentence)\n",
    "\n",
    "input_x_fra = onehot_encode(input_sequence_fra, max_encoder_seq_length_fra, num_encoder_tokens_fra)\n",
    "input_x_ita = onehot_encode(input_sequence_ita, max_encoder_seq_length_ita, num_encoder_tokens_ita)\n",
    "\n",
    "translated_sentence_fra, translated_sentence_ita = decode_sequence(input_x_fra, input_x_ita)\n",
    "# translated_sentence = decode_sequence(input_x_ita)\n",
    "\n",
    "print('source sentence is            : ' + input_sentence)\n",
    "print('translated sentence French is : ' + translated_sentence_fra)\n",
    "print('translated sentence Italian is: ' + translated_sentence_ita)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Calculate BLUE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_textt_fra  = [\"\" for x in range(len(test_encoder_input_data_fra))]\n",
    "target_textt_fra = [\"\" for x in range(len(test_encoder_input_data_fra))]\n",
    "decode_textt_fra = [\"\" for x in range(len(test_encoder_input_data_fra))]\n",
    "\n",
    "input_textt_ita  = [\"\" for x in range(len(test_encoder_input_data_ita))]\n",
    "target_textt_ita = [\"\" for x in range(len(test_encoder_input_data_ita))]\n",
    "decode_textt_ita = [\"\" for x in range(len(test_encoder_input_data_ita))]\n",
    "\n",
    "for seq_index in range(len(test_encoder_input_data_fra)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq_fra = test_encoder_input_data_fra[seq_index: seq_index + 1]\n",
    "    input_seq_ita = test_encoder_input_data_ita[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence_fra, decoded_sentence_ita = decode_sequence(input_seq_fra, input_seq_ita)\n",
    "    \n",
    "    input_textt_fra[seq_index]  = input_texts_fra[seq_index+158999]\n",
    "    target_textt_fra[seq_index] = target_texts_fra[seq_index+158999][1:-1]\n",
    "    decode_textt_fra[seq_index] = decoded_sentence_fra[0:-1]\n",
    "    \n",
    "    input_textt_ita[seq_index]  = input_texts_ita[seq_index+158999]\n",
    "    target_textt_ita[seq_index] = target_texts_ita[seq_index+158999][1:-1]\n",
    "    decode_textt_ita[seq_index] = decoded_sentence_ita[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        i can see youre busy so ill get right to the point\n",
      "French (true):  je peux voir que vous etes occupe donc je vais aller droit au but\n",
      "French (pred):  je ne peux pas vous rendre a la maison au sujet de la plage\n",
      "-\n",
      "English:        i can tell you things you wont hear from anyone else\n",
      "French (true):  je peux te raconter des choses que tu nentendras de personne dautre\n",
      "French (pred):  je ne peux pas me preter un peu de sentiment a la maison aujourdhui\n",
      "-\n",
      "English:        i cant afford to buy a used car much less a new one\n",
      "French (true):  je ne peux pas me permettre dacheter une voiture doccasion et encore moins une nouvelle\n",
      "French (pred):  je narrive pas a croire que je suis un peu de sens de la plage\n",
      "-\n",
      "English:        i cant believe we havent run into each other before\n",
      "French (true):  je narrive pas a croire que nos chemins ne se soient pas croises auparavant\n",
      "French (pred):  je narrive pas a croire que vous ne soyez pas de conseils de tout ce que tu as dit\n",
      "-\n",
      "English:        i cant believe you didnt recognize your own brother\n",
      "French (true):  je narrive pas a croire que tu naies pas reconnu ton propre frere\n",
      "French (pred):  je narrive pas a croire que vous ne soyez pas de conseils de tout ce que tu as dit\n",
      "-\n",
      "English:        i cant believe you didnt recognize your own brother\n",
      "French (true):  je narrive pas a croire que vous nayez pas reconnu votre propre frere\n",
      "French (pred):  je narrive pas a croire que vous ne soyez pas tres contrariees a partir de votre probleme\n",
      "-\n",
      "English:        i cant believe you dont want butter on your popcorn\n",
      "French (true):  je narrive pas a croire que tu ne veuilles pas de beurre sur ton popcorn\n",
      "French (pred):  je narrive pas a croire que tu sois plus de la piece a la fete\n",
      "-\n",
      "English:        i cant believe youre taking pictures of cockroaches\n",
      "French (true):  je narrive pas a croire que tu prends des cliches de cafards\n",
      "French (pred):  je narrive pas a croire que je suis seul a la maison a manger un peu de sempine\n",
      "-\n",
      "English:        i cant conceive how i could have made such a mistake\n",
      "French (true):  je ne comprends pas comment jai pu faire une erreur pareille\n",
      "French (pred):  je narrive pas a croire que je suis a la maison au condeil de la piece\n",
      "-\n",
      "English:        i cant figure out how to post a comment to this blog\n",
      "French (true):  je narrive pas a trouver comment publier un commentaire sur ce journal\n",
      "French (pred):  je ne peux pas le faire de la plus proche de la plusiere de tom\n",
      "-\n",
      "English:        i cant put up with the inconvenience of country life\n",
      "French (true):  je narrive pas a supporter les desagrements de la vie a la campagne\n",
      "French (pred):  je ne peux pas me preter le meme chaque jour de la plage\n",
      "-\n",
      "English:        i cant remember her address no matter how much i try\n",
      "French (true):  peu importe a quel point jessaie je narrive pas a me rappeler son adresse\n",
      "French (pred):  je narrive pas a croire que ce soit de ma vie de ma chambre\n",
      "-\n",
      "English:        i cant solve this problem its too difficult for me\n",
      "French (true):  je ne peux pas resoudre ce probleme il est trop difficile pour moi\n",
      "French (pred):  je ne peux pas vous aider a manger que je peux etre allee aujourdhui\n",
      "-\n",
      "English:        i cant tell you any more ive already said too much\n",
      "French (true):  je ne peux pas ten dire plus jen ai deja trop dit\n",
      "French (pred):  je ne peux pas vous aider a manger que je peux etre allee aujourdhui\n",
      "-\n",
      "English:        i cant tell you any more ive already said too much\n",
      "French (true):  je ne peux pas vous en dire plus jen ai deja trop dit\n",
      "French (pred):  je ne peux pas te dire que tu as besoin de me laisser le francais\n",
      "-\n",
      "English:        i cant thank you enough for the help youve given me\n",
      "French (true):  je ne sais assez vous remercier pour laide que vous mavez fournie\n",
      "French (pred):  je ne peux pas te dire que tu as besoin de me laisser le francais\n",
      "-\n",
      "English:        i cant thank you enough for the help youve given me\n",
      "French (true):  je ne sais assez te remercier pour laide que tu mas fournie\n",
      "French (pred):  je ne peux pas me prementer le probleme avec le probleme\n",
      "-\n",
      "English:        i cannot disclose any information about the informant\n",
      "French (true):  je ne peux devoiler aucune information au sujet de linformateur\n",
      "French (pred):  je me suis rentre chez moi de la maison a ce que je peux faire\n",
      "-\n",
      "English:        i carefully took down everything that my teacher said\n",
      "French (true):  jai soigneusement note toutes les choses que le professeur avait dites\n",
      "French (pred):  jai appele le probleme avec le probleme de la maison\n",
      "-\n",
      "English:        i changed the arrangement of the furniture in my room\n",
      "French (true):  jai change la disposition des meubles de ma piece\n",
      "French (pred):  je lai conficiee a ce que je peux le faire a manger quelque chose\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(20):\n",
    "    print('-')\n",
    "    print('English:       ', input_textt_fra[seq_index])\n",
    "    print('French (true): ', target_textt_fra[seq_index])\n",
    "    print('French (pred): ', decode_textt_fra[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        on sunday i go to church\n",
      "Italian (true):  io vado in chiesa la domenica\n",
      "Italian (pred):  si sono perso il tuo numero\n",
      "-\n",
      "English:        on your mark get set go\n",
      "Italian (true):  pronti partenza via\n",
      "Italian (pred):  il suo distorso e profondo\n",
      "-\n",
      "English:        one cup of coffee please\n",
      "Italian (true):  una tazza di caffe per favore\n",
      "Italian (pred):  il suo distorso e profondo\n",
      "-\n",
      "English:        one cup of coffee please\n",
      "Italian (true):  una tazza di caffe per piacere\n",
      "Italian (pred):  il suo distorso e profondo\n",
      "-\n",
      "English:        one cup of coffee please\n",
      "Italian (true):  una tazzina di caffe per favore\n",
      "Italian (pred):  il suo distorso e profondo\n",
      "-\n",
      "English:        one cup of coffee please\n",
      "Italian (true):  una tazzina di caffe per piacere\n",
      "Italian (pred):  oggi con mangiate state aspettane\n",
      "-\n",
      "English:        one day youll understand\n",
      "Italian (true):  un giorno capirete\n",
      "Italian (pred):  oggi con mangiate state aspettane\n",
      "-\n",
      "English:        one day youll understand\n",
      "Italian (true):  un giorno capirai\n",
      "Italian (pred):  oggi con mangiate state aspettane\n",
      "-\n",
      "English:        one day youll understand\n",
      "Italian (true):  un giorno capira\n",
      "Italian (pred):  non ha fatto male la sua parte\n",
      "-\n",
      "English:        one hand washes the other\n",
      "Italian (true):  una mano lava laltra\n",
      "Italian (pred):  il suo disto sono preoccupata presto\n",
      "-\n",
      "English:        one lump of sugar please\n",
      "Italian (true):  una zolletta di zucchero per favore\n",
      "Italian (pred):  il suo disto sono preoccupata presto\n",
      "-\n",
      "English:        one lump of sugar please\n",
      "Italian (true):  una zolletta di zucchero per piacere\n",
      "Italian (pred):  a compranda qui per piacere\n",
      "-\n",
      "English:        one must follow the rules\n",
      "Italian (true):  bisogna seguire le regole\n",
      "Italian (pred):  a me non piace parlare di nuovo\n",
      "-\n",
      "English:        one vice leads to another\n",
      "Italian (true):  un vizio conduce allaltro\n",
      "Italian (pred):  a me non piace parlare di nuovo\n",
      "-\n",
      "English:        one vice leads to another\n",
      "Italian (true):  un vizio tira laltro\n",
      "Italian (pred):  solo tom posso fare da solo\n",
      "-\n",
      "English:        only tom can talk to mary\n",
      "Italian (true):  solo tom puo parlare con mary\n",
      "Italian (pred):  solo tom posso fare da solo\n",
      "-\n",
      "English:        only tom can talk to mary\n",
      "Italian (true):  soltanto tom puo parlare con mary\n",
      "Italian (pred):  solo tom posso fare da solo\n",
      "-\n",
      "English:        only tom can talk to mary\n",
      "Italian (true):  solamente tom puo parlare con mary\n",
      "Italian (pred):  solo tom posso fare da solo\n",
      "-\n",
      "English:        only tom can talk to mary\n",
      "Italian (true):  solo tom riesce a parlare con mary\n",
      "Italian (pred):  solo tom posso fare da solo\n",
      "-\n",
      "English:        only tom can talk to mary\n",
      "Italian (true):  soltanto tom riesce a parlare con mary\n",
      "Italian (pred):  solo tom posso fare da solo\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(20):\n",
    "    print('-')\n",
    "    print('English:       ', input_textt_ita[seq_index])\n",
    "    print('Italian (true): ', target_textt_ita[seq_index])\n",
    "    print('Italian (pred): ', decode_textt_ita[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates(lst, item):\n",
    "    return [i for i, x in enumerate(lst) if x == item]\n",
    "\n",
    "length_fra = numpy.zeros([780,1])\n",
    "dupi_fra   = numpy.zeros([780,14], dtype=numpy.int32)\n",
    "\n",
    "j = 0; k = 0;\n",
    "for i in range(780):\n",
    "    dup_fra = duplicates(input_textt_fra, input_textt_fra[k])\n",
    "    dupi_fra[i,:len(dup_fra)] = dup_fra\n",
    "    length_fra[i] = len(dup_fra)\n",
    "    k+=len(dup_fra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21310638162912765"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "score_fra = numpy.zeros([780,1])\n",
    "\n",
    "for i in range(780):\n",
    "    vv = numpy.nonzero(dupi_fra[i])\n",
    "    texttt = [\"\" for x in range(len(vv))]\n",
    "    for j in range(len(vv)):\n",
    "        texttt[j] = target_textt_fra[dupi_fra[i, j]]\n",
    "    score_fra[i] = sentence_bleu(texttt, decode_textt_fra[dupi_fra[i,0]])\n",
    "\n",
    "numpy.average(score_fra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates(lst, item):\n",
    "    return [i for i, x in enumerate(lst) if x == item]\n",
    "\n",
    "length_ita = numpy.zeros([457,1])\n",
    "dupi_ita   = numpy.zeros([457,12], dtype=numpy.int32)\n",
    "\n",
    "j = 0; k = 0;\n",
    "for i in range(457):\n",
    "    dup_ita = duplicates(input_textt_ita, input_textt_ita[k])\n",
    "    dupi_ita[i,:len(dup_ita)] = dup_ita\n",
    "    length_ita[i] = len(dup_ita)\n",
    "    k+=len(dup_ita)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17261413213035998"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "score_ita = numpy.zeros([457,1])\n",
    "\n",
    "for i in range(457):\n",
    "    vv = numpy.nonzero(dupi_ita[i])\n",
    "    texttt = [\"\" for x in range(len(vv))]\n",
    "    for j in range(len(vv)):\n",
    "        texttt[j] = target_textt_ita[dupi_ita[i, j]]\n",
    "    score_ita[i] = sentence_bleu(texttt, decode_textt_ita[dupi_ita[i,0]])\n",
    "\n",
    "numpy.average(score_ita)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
